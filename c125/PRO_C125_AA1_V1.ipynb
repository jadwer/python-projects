{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RL Problema a resolver:\n",
        "\n",
        "La **máquina/bot** necesita encontrar una forma de alcanzar el **objetivo** (sala número 5). La máquina empezará desde cualquier sala al azar.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1PBhfah4fz8CYulyn9BzwBcH91nGO6ouS\" width= 400>"
      ],
      "metadata": {
        "id": "gbWq1YuSKHYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Componentes de aprendizaje por refuerzo (RL):\n",
        "\n",
        "* Entorno\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/af5a1d77-041a-4bba-be4e-ca26aea97771.png\" width= 300>\n",
        "\n",
        "* Agente\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/fda2e793-0302-420c-981b-dbf32ecb1d12.png\" width= 200>\n",
        "\n",
        "* Estados\n",
        "\n",
        "> El agente/máquina puede estar en cualquiera de las 6 salas (5 salas + 1 sala objetivo) de la tienda. Por lo tanto, hay **6 estados posibles**.\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?export=view&id=1OC4gRJfhrHkM5SyIVLQsgzI6HOTnDWzr\" width= 50>\n",
        "\n",
        "\n",
        "* Acciones\n",
        "\n",
        "> El agente/máquina puede escoger moverse a cualquiera de las 6 salas (5 salas + 1 sala objetivo) de la tienda. Por lo tanto, también hay **6 acciones posibles**.\n",
        "\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?export=view&id=1DTHWqcMgoNV6W4oJafXyZO7xAdidOXFZ\" width= 300>\n",
        "\n",
        "\n",
        "* Recompensas\n",
        "\n",
        "> **Recompensas posibles: -1, 0, 100**\n",
        "\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?export=view&id=19-PzHY_OLu6rFkFZwu37fD0g6eynaVfy\" width= 300>\n",
        "\n",
        "* **No se puede mover**: si no hay un camino directo de una habitación a otra, entonces la recompensa es -1.\n",
        "\n",
        "* **Mover**: si la máquina puede moverse de la sala actual (estado) a la siguiente sala (acción), la recompensa es 0.\n",
        "\n",
        "* **Meta**: si la máquina está o alcanzó el objetivo, la recompensa es 100.\n",
        "\n"
      ],
      "metadata": {
        "id": "UfO6LjShKUx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar módulos"
      ],
      "metadata": {
        "id": "IoIK1JeQKecl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WKZ1AbHZowja"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de recompensas\n",
        "\n",
        "> Haz clic en el enlace [matriz de recompensas](https://drive.google.com/file/d/1T7LPuYN-_bebPDTx-htTvWMiNgHor_Jg/view?usp=sharing) para entender la asignación de recompensas.\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?id=1xvsTacakR3UB1AK5Xnk8201Yr98ky6N4\" width= 400>\n",
        "\n",
        "* **No se puede mover**: si no hay un camino directo de una habitación a otra, entonces la recompensa es -1.\n",
        "\n",
        "* **Mover**: si la máquina puede moverse de la sala actual (estado) a la siguiente sala (acción), la recompensa es 0.\n",
        "\n",
        "* **Meta**: si la máquina está o alcanzó el objetivo, la recompensa es 100.\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?id=1yMXYjRTFEbXDDFmolj4Qp2p63eWzKkiY\" width= 400>"
      ],
      "metadata": {
        "id": "2aeQKNY4Ku9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = np.array([\n",
        "    [-1, -1, -1, -1,  0,  -1],\n",
        "    [-1, -1, -1,  0, -1, 100],\n",
        "    [-1, -1, -1 , 0 ,-1 , -1],\n",
        "    [-1,  0,  0 ,-1  ,-1 , -1],\n",
        "    [0, -1, -1 , -1 ,-1 ,100],\n",
        "    [-1, -1, -1, -1,  0, 100]\n",
        "])"
      ],
      "metadata": {
        "id": "6RjBxy7yo4fg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estado inicial"
      ],
      "metadata": {
        "id": "GTj7j-a6K3S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_initial_state():\n",
        "    return np.random.randint(0, 6)"
      ],
      "metadata": {
        "id": "S-Qe6w--o641"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_initial_state()"
      ],
      "metadata": {
        "id": "tC0B10aQpGdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8156909-ae24-4e15-8c5c-baf21ca7bb1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener acción"
      ],
      "metadata": {
        "id": "Cv3ucAIxLPFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(current_state, reward_matrix):\n",
        "    available_action = []\n",
        "    print(\"Matriz de recompensas\",\"\\n\",reward_matrix)\n",
        "    for action in enumerate(reward_matrix[current_state]): # [[0,-1],[1,-1] [2,-1],  [3, 0], [4,-1], [5, 100]]\n",
        "        if action[1]!= -1: # [indice, accion[1]]\n",
        "            available_action.append(action[0]) # [accion[0], valor], se agrega el indice si la accion es diferente a -1\n",
        "    choose_action = random.choice(available_action)\n",
        "    print(\"Estado actual\",current_state)\n",
        "    print(\"La elección aleatoria de la acción en\",available_action,\"es\", choose_action)\n",
        "    return choose_action"
      ],
      "metadata": {
        "id": "3D9SYz0CpBkk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_state = 0\n",
        "\n",
        "choosen_action = get_action(current_state, rewards)"
      ],
      "metadata": {
        "id": "-A5-F_41pOFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936c5ce5-cd93-4806-fc0f-9d83c680c756"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de recompensas \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Estado actual 0\n",
            "La elección aleatoria de la acción en [4] es 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz-Q\n",
        "**Q-learning** es un algoritmo de aprendizaje por refuerzo (RL). Dado un estado actual, ayuda a encontrar la mejor acción que el agente puede tomar.\n",
        "\n",
        "La 'Q' significa 'Quality', es decir, 'Calidad'. La calidad representa que tan útil es una acción para ganar una recompensa.\n",
        "\n",
        "Para realizar la técnica Q-learning, usamos una **matriz-Q**. También se encuentra en forma de estados en las filas y acciones en las columnas. Inicialmente, todos los elementos de la matriz-Q son ceros."
      ],
      "metadata": {
        "id": "WZ-8DI1ug6MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una matriz-Q aquí\n",
        "\n",
        "q_matrix = np.zeros([6,6])\n",
        "print(q_matrix)"
      ],
      "metadata": {
        "id": "MWPWggL4g5Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0c6a8b-a7ba-444c-d58e-3150a91fcf13"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realizar una acción\n",
        "\n",
        "[Matriz-Q](https://drive.google.com/file/d/1tBvfDI5L515E-t4KstcjNk1eIYJRx1w8/view?usp=sharing )"
      ],
      "metadata": {
        "id": "eKQNleMdiA7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.8\n",
        "\n",
        "def take_action(current_state, reward_matrix, gamma) :\n",
        "  action = get_action(current_state, reward_matrix)\n",
        "\n",
        "  # Estado, acción y recompensa actuales\n",
        "  current_sa_reward = reward_matrix[current_state, action]\n",
        "  print(\"Estado actual de la acción tomada: \", current_sa_reward)\n",
        "\n",
        "  # Estado, acción y recompensa nuevos\n",
        "  q_sa_value = max(q_matrix[action,])\n",
        "  print(\"q_sa_value: \", q_sa_value)\n",
        "\n",
        "  # Estado Q actual\n",
        "  q_current_state = current_sa_reward + (gamma * q_sa_value)\n",
        "  print(\"Estado actual de Q:\", q_current_state)\n",
        "\n",
        "  # Actualizar la matriz Q\n",
        "  q_matrix[current_state, action] = q_current_state\n",
        "\n",
        "  print(\"Matriz-Q\", \"\\n\", q_matrix)"
      ],
      "metadata": {
        "id": "bFvS6vLwABLR"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_state = 0\n",
        "take_action(current_state, rewards, gamma)"
      ],
      "metadata": {
        "id": "1e-o8gDDAnnW",
        "outputId": "888eda7b-043e-48b4-fadf-5859e66b873a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de recompensas \n",
            " [[ -1  -1  -1  -1   0  -1]\n",
            " [ -1  -1  -1   0  -1 100]\n",
            " [ -1  -1  -1   0  -1  -1]\n",
            " [ -1   0   0  -1  -1  -1]\n",
            " [  0  -1  -1  -1  -1 100]\n",
            " [ -1  -1  -1  -1   0 100]]\n",
            "Estado actual 0\n",
            "La elección aleatoria de la acción en [4] es 4\n",
            "Estado actual de la acción tomada:  0\n",
            "q_sa_value:  164.0\n",
            "Estado actual de Q: 131.20000000000002\n",
            "Matriz-Q \n",
            " [[  0.    0.    0.    0.  131.2   0. ]\n",
            " [  0.    0.    0.   64.    0.  100. ]\n",
            " [  0.    0.    0.   64.    0.    0. ]\n",
            " [  0.   80.    0.    0.    0.    0. ]\n",
            " [ 64.    0.    0.    0.    0.  164. ]\n",
            " [  0.    0.    0.    0.   80.    0. ]]\n"
          ]
        }
      ]
    }
  ]
}